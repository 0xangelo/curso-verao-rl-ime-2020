{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 2 - Parte Prática - Policy Gradients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "Nesse segundo notebook vamos aprender como melhorar o desempenho de um agente ao longo do tempo com base em experiências.\n",
    "\n",
    "A abordagem que iremos estudar é conhecida por **Otimização de Política** baseada em gradientes (outro nome comum encontrado na literatura é Busca de Política (*Policy Search*)). De maneira geral, essa classe de algoritmos de RL é denominada por **Policy Gradients**.\n",
    "\n",
    "Começaremos a explorar Policy Gradients nesse notebook com uma implementação básica. A fim de desenvolver gradualmente seus principais componentes e entender o papel de cada um deles, iremos de forma didática construir o algoritmo **REINFORCE**.\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left [ \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(\\mathbf{a}_t|\\mathbf{s}_t)  \\sum_{t'=0}^{T-1} r_{t'} \\right ]\n",
    "$$\n",
    "\n",
    "\n",
    "### Objetivos:\n",
    "\n",
    "- Entender a abordagem de otimização de políticas como busca no espaço de parâmetros da política\n",
    "- Implementar um primeiro agente baseado no algoritmo REINFORCE\n",
    "- Familiarizar-se com a API básica de construção de modelos (i.e., redes neurais) em Keras\n",
    "- Familiarizar-se com métodos de Deep Learning usando TensorFlow 2.X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "> **Atenção:** não se esqueça de executar todos os `imports` necessários antes prosseguir com o tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "from utils.agent import RLAgent\n",
    "from utils.memory import OnPolicyReplay\n",
    "import utils.runner\n",
    "from utils.viz import *\n",
    "\n",
    "# sanity check\n",
    "assert tf.__version__ == '2.1.0'\n",
    "assert tf.executing_eagerly()\n",
    "\n",
    "tf.get_logger().setLevel('ERROR') # ignore TensorFlow warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Redes Neurais em TensorFlow e Keras\n",
    "\n",
    "Antes de começarmos propriamente com a implementação do algoritmo REINFORCE visto em aula. Iremos introduzir em linhas gerais a biblioteca de construção de modelos (i.e., redes neurai artificiais) do Keras no TensorFlow.\n",
    "\n",
    "A grosso modo, a API do Keras é uma camada de *software* implementada em cima das funções, classes e métodos dos pacotes de Deep Learning com o objetivo de facilitar a implementação, treinameto e avaliação de redes neurais. Maiores detalhes sobre a especificação do Keras podem ser acessadas em [https://keras.io/](https://keras.io/). A documentação específica da implementação do Keras para TensorFlow se encontra em [https://www.tensorflow.org/guide/keras](https://www.tensorflow.org/guide/keras).\n",
    "\n",
    "Note que a biblioteca do Keras é bastante extensa e flexível, de forma que nesse tutorial nos restringiremos a apresentar na medida do necessário os conceitos básicos necessários para trabalhar com redes neurais no contexto de Aprendizado por Reforço, e em particular, com foco nas primitivas necessárias para se implementar Policy Gradients.\n",
    "\n",
    "> **Observação**: para aqueles interessados em uma introdução mais detalhada, sugerimos seguir após a aula os tutoriais do Tensorflow (e.g., https://www.tensorflow.org/tutorials/quickstart/beginner).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Camadas e Modelos Sequenciais\n",
    "\n",
    "A rede neural mais simples que podemos construir é conhecida por *[Multi-Layer Perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron)* (MLP). Também conhecida por *Feedforward* ou *Fully-connected Network*.\n",
    "\n",
    "Uma MLP é definida pela composição de camadas de transformações afins $h: \\mathbb{R}^q \\rightarrow \\mathbb{R}^p$ seguidas por uma função não-linear (e.g., *activation*):\n",
    "$\n",
    "h(\\mathbf{x}) = \\text{activation}(\\mathbf{W}_{p \\times q} \\cdot \\mathbf{x}_{q \\times 1} + \\mathbf{b}_{p \\times 1})\n",
    "$ .\n",
    "\n",
    "Recentemente, tem se tornado bastante comum o uso da função de ativação ReLu (*Rectified Linear Unit*):\n",
    "$\n",
    "\\text{ReLu}(x) = max(0, x)\n",
    "$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Keras podemos utilizar a classe `tf.keras.layers.Dense` para construir a função $h(\\cdot)$ e a classe `tf.keras.Sequential` para encadear essas transformações (comumente chamadas de *camadas*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, input_shape=(5,), activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(3, activation=\"relu\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(size=(5,))\n",
    "y = mlp(x[None,:])\n",
    "\n",
    "print(f\">> mlp(x) transforma vetores `x` de tamanho {x.shape[-1]} em vetores `y` de tamanho {y.shape[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É importante observar que camadas e modelos em Keras esperam receber vetores em *batch*. Dessa forma, é possível processar conjuntos de vetores em paralelo; o que torna a computação consideravelmente mais eficiente.\n",
    "\n",
    "No exemplo anterior, a variável `x = np.random.normal(size=(5,))` representa um único vetor de tamanho 5. Dessa forma, é necessário adicionar uma dimensão extra através do truque de manipulação de tensores `x[None,:]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(size=(5,))\n",
    "assert x.shape == (5,)\n",
    "assert x[None,:].shape == (1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para processar vetores em *batch*, o Keras assume que a primeira dimensão do vetor corresponde ao `batch_size`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "x = np.random.normal(size=(batch_size, 5))\n",
    "y = mlp(x)\n",
    "\n",
    "print(f\">> mlp(x) transformou {x.shape[0]} vetores `x` de tamanho {x.shape[-1]} em vetores {y.shape[0]} `y` de tamanho {y.shape[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Camadas Probabilísticas\n",
    "\n",
    "Em Aprendizado por Reforço é muitas vezes necessário representar distribuições (i.e., variáveis aleatórias). Por exemplo, na próxima seção utilizaremos uma distribuições para representar política estocásticas definida para um espaço discreto de ações.\n",
    "\n",
    "Em TensorFlow 2.X é fácil representar as principais distribuições através da biblioteca `tensorflow_probability` (https://www.tensorflow.org/probability).\n",
    "\n",
    "> **Observação**: para os interessados em modelos probabilísticos e inferência, recomendamos revisar após a aula o tutorial sobre regressão usando o TensorFlow Probability disponível em https://blog.tensorflow.org/2019/03/regression-with-probabilistic-layers-in.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esse tutorial precisaremos representar **distribuições categóricas** (https://en.wikipedia.org/wiki/Categorical_distribution) de forma a amostrar valoes discretos ou classes (representadas por números inteiros) de acordo com um vetor de probabilidades.\n",
    "\n",
    "Lembre-se que para uma variável $x \\in \\{1, 2, \\cdots, K\\}$ e um vetor de probabilidades $[p_1, p_2, \\cdots, p_K]$ (i.e., $p_i \\in [0, 1], \\sum_{i=1}^K p_i = 1$), temos que:\n",
    "\n",
    "$$\n",
    "x \\sim \\text{Categorical}([p_1, p_2, \\cdots, p_K]) \\Leftrightarrow P(x = i) = p_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No TensorFlow podemos adicionar um camada probabilística através da classe `tfp.layers.DistributionLambda`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, input_shape=(5,), activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(4),\n",
    "    tfp.layers.DistributionLambda(lambda t: tfd.Categorical(logits=t))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se invocarmos a rede neural implementada pela variável `model` em um conjunto de vetores de entrada `inputs`, observamos que a saída corresponde a um objeto do tipo `tfp.distributions.Categorical` e não a valores da variável aleatória:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.random.normal(size=(8, 5))\n",
    "\n",
    "dist = model(inputs)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Atenção**: observe como o modelo representa um distribuição parametrizada (e.g., os parâmetros de uma categórica são conhecidos por`logits`). As primeiras camadas do tipo `Dense` processam os vetores de entrada gerando um conjunto de vetores de *logits* usados para definir a distribuição condicional categórica. Em outras palavras, para cada vetor de entrada temos uma distribuição categórica (por isso dizemos que o modelo representa uma distribuição condicional, isto é, condicionada no vetor de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para amostrar realizações da variável aleatória dada pela distribuição categórica, utilizamos o método `sample()` (disponível para todas as distribuições definidas no TensorFlow Probability):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dist.sample()\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Atenção**: note que embora a entrada do `model` seja um array n-dimensional no NumPy, uma amostra da distribuição é um objeto `Tensor` do TensorFlow. Você pode usar o método `numpy()` para transformar `tf.Tensor` em um `np.ndarray`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outro método importante para se familiarizar é `log_prob`. Dada uma amostra ou vetores de amostra, o método `log_prob` devolve o logaritmo da probabilidade de se ter gerado a amostra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_log_prob = dist.log_prob(sample)\n",
    "print(sample_log_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Atenção**: o método `log_prob` será fundamental na implementação dos algoritmos de *Policy Gradientes* nessse curso. Tenha certeza que você compreendeu seu funcionamento em termos de entradas e saídas da função."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementando políticas estocásticas em tf.Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse tutorial iremos trabalhar com problemas do OpenAI gym que são definidos para um espaço discreto de ações. Dessa forma, precisaremos representar políticas estocásticas  $\\pi_\\theta(\\mathbf{a}_t | \\mathbf{s}_t)$ para ações discretas. Para isso, utilizaremos redes neurais construídas via Keras.\n",
    "\n",
    "Para não perdermos tempo com detalhes de implementação, disponibilizamos a função `build_discrete_policy` que recebe as especificações dos espaços de observação e ação do ambiente e o hiper-parâmetros que definem a arquitetura da rede neural:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discrete_policy(obs_space, action_space, hidden_layers, activation=\"relu\"):\n",
    "    Input = tf.keras.Input\n",
    "    Dense = tf.keras.layers.Dense\n",
    "    DistributionLambda = tfp.layers.DistributionLambda\n",
    "    Categorical = tfd.Categorical\n",
    "    \n",
    "    policy_net_layers = []\n",
    "\n",
    "    policy_net_layers.append(Input(shape=obs_space.shape, name=\"State\"))\n",
    "\n",
    "    for i, units in enumerate(hidden_layers):\n",
    "        policy_net_layers.append(Dense(units=units, activation=activation, name=f\"Hidden{i+1}\"))\n",
    "    \n",
    "    policy_net_layers.append(Dense(units=action_space.n, name=\"Logits\"))\n",
    "    policy_net_layers.append(DistributionLambda(lambda t: Categorical(logits=t), name=\"Action_Distribution_Categorical\"))\n",
    "                                     \n",
    "    return tf.keras.Sequential(policy_net_layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute o código abaixo para criar um modelo com 2 camadas de 64 unidades utilizando o `relu` como função de ativação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "hidden_layers = [64, 64]\n",
    "activation = \"relu\"\n",
    "\n",
    "policy = build_discrete_policy(env.observation_space, env.action_space, hidden_layers, activation)\n",
    "\n",
    "obs = env.observation_space.sample()\n",
    "action_dist = policy(obs[None,:])\n",
    "\n",
    "action = action_dist.sample().numpy()\n",
    "assert action[0] in env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode inspecionar visualmente a arquitetura da política (e de qualquer modelo no Keras) através da função `tf.keras.utils.plot_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(policy, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Agente REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez que já nos famialiarizamos com a construção de modelos via Keras e com as particularidades de uma política estocástica implementada como modelo probabilística (via distribuição categórica), podemos começar a implementar a versão inicial do algoritmo *REINFORCE*. Lembre-se que, pela teoria desenvolvida em aula, o gradiente utilizado para otimizar a política é dado por:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left [ \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(\\mathbf{a}_t|\\mathbf{s}_t) \\hat{R}(\\tau)\\right ]~,\n",
    "$$\n",
    "onde $\\hat{R}(\\tau) = \\sum_{t=0}^{T-1} r_t$ é retorno de uma trajetória simulada no ambiente\n",
    "$\n",
    "\\tau = (\\mathbf{s}_0, \\mathbf{a}_0, r_0, \\mathbf{s}_1, \\mathbf{a}_1, r_1, \\cdots, \\mathbf{s}_{T-1}, \\mathbf{a}_{T-1}, r_{T-1}) \\sim \\pi_\\theta.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como não é possível calcular analiticamente esse valor esperado no caso geral, estimaremos via **Monte-Carlo** o gradiente da política:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{k=1}^K \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(\\mathbf{a}_t^{(k)}|\\mathbf{s}_t^{(k)}) \\hat{R}(\\tau^{(k)})~,\n",
    "$$\n",
    "\n",
    "onde o índice $k$ denota vetores da $k$-ésima trajetória amostrada. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Surrogate Loss e diferenciação automática\n",
    "\n",
    "Para usar a diferenciação automática no TensorFlow (i.e., computar gradientes via `tape.gradient`) é necessário definir uma função objetivo. No contexto de aprendizado de máquina no geral, essa função objetivo é tipicamente chamada de *loss* e a maioria dos pacotes de *Deep Learning* tenta **minimizar essa função**.\n",
    "\n",
    "Dessa forma, como estamos querendo maximizar o desempenho do agente denotado por $J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} [R(\\tau)]$ deveremos minimizar o negativo de $J(\\theta)$. Para o Policy Gradients a função de *loss* que vamos utilizar é dada por:\n",
    "\n",
    "$$\n",
    "\\hat{J}(\\theta) = \\frac{1}{N} \\sum_{k=1}^K \\sum_{t=0}^{T-1}  \\log \\pi_\\theta(\\mathbf{a}_t^{(k)}|\\mathbf{s}_t^{(k)}) \\hat{R}(\\tau^{(k)})~.\n",
    "$$\n",
    "\n",
    "> **IMPORTANTE**: note que em geral $\\hat{J}(\\theta) \\neq J(\\theta)$, no entanto, $\\nabla_\\theta \\hat{J}(\\theta) \\approx \\nabla_\\theta J(\\theta)$. Essa é a razão pela qual $\\hat{J}$ é chamada de *surrogate loss*; *surrogate* em português significa \"substituto\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**<font color=\"red\">EXERCÍCIO-PROGRAMA 1:</font>**\n",
    "\n",
    "Nesse exercício você deverá codificar o método `REINFORCE._loss_fn` que será reponsável por implementar a função de *loss* do *Policy Gradient*.\n",
    "\n",
    "> **Nota 1**: observe que cada `batch` corresponde a um dicionário de listas, onde cada lista corresponde a um conjunto de episódios. Por exemplo, `batch[\"states\"][k]` devolve os estados do primeiro k-episódio amostrado.\n",
    "\n",
    "> **Nota 2**: pode ser útil utilizar as funções de agregação `np.sum` e `tf.reduce_sum`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE(RLAgent):\n",
    "    \n",
    "    def __init__(self, obs_space, action_space, config=None):\n",
    "        super(REINFORCE, self).__init__(obs_space, action_space, config)\n",
    "        \n",
    "        self.memory = OnPolicyReplay()\n",
    "        self.policy = build_discrete_policy(self.obs_space, self.action_space, config[\"hidden_layers\"], config[\"activation\"])\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=config[\"learning_rate\"])\n",
    "\n",
    "    def act(self, obs):\n",
    "        return self._act(obs).numpy()\n",
    "    \n",
    "    @tf.function\n",
    "    def _act(self, obs):\n",
    "        action_dist = self.policy(obs[None,:])\n",
    "        return action_dist.sample()[0]\n",
    "\n",
    "    def observe(self, obs, action, reward, next_obs, done):\n",
    "        self.memory.update(obs, action, reward, next_obs, done)\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.batch_size < self.config[\"train_batch_size\"]:\n",
    "            return\n",
    "        \n",
    "        batch = self.memory.sample()\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._loss_fn(batch)\n",
    "            gradients = tape.gradient(loss, self.policy.trainable_weights)\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.policy.trainable_weights))\n",
    "        \n",
    "        grad_norm = [(w.name, tf.norm(grad).numpy()) for w, grad in zip(self.policy.trainable_weights, gradients)]\n",
    "      \n",
    "        return loss.numpy(), grad_norm\n",
    "\n",
    "    def _loss_fn(self, batch):\n",
    "        states, actions, rewards = batch[\"states\"], batch[\"actions\"], batch[\"rewards\"]\n",
    "        n_episodes = len(states)\n",
    "\n",
    "        loss = 0.0\n",
    "        \n",
    "        # SEU CÓDIGO AQUI ===================================\n",
    "        for episode in range(n_episodes):\n",
    "            action_dist = self.policy(states[episode])\n",
    "            log_probs = action_dist.log_prob(actions[episode])\n",
    "            R = np.sum(rewards[episode])\n",
    "            loss += - tf.reduce_sum(log_probs * R)\n",
    "            \n",
    "        loss /= n_episodes\n",
    "        # ===================================================\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para testar a sua implementação, rode o código abaixo. Para os parâmetros definidos abaixo no dicionário de `config`, você deve obter um `avg_loss` entre 350.0 e 450.0. Note que estamos usando simulação de Monte-Carlo então o resultado pode variar um pouco.\n",
    "\n",
    "> **IMPORTANTE**: caso a sua implementação não esteja passando no teste abaixo (em várias tentativas consecutivas) e você achar que sua implementação está correta, chame o monitor ou um dos instrutores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "config = {\n",
    "    # policy net\n",
    "    \"hidden_layers\": [64, 64],\n",
    "    \"activation\": \"relu\",\n",
    "\n",
    "    # optimization\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"learning_rate\": 1e-3,\n",
    "\n",
    "    # training\n",
    "    \"train_batch_size\": 3000,\n",
    "}\n",
    "\n",
    "agent = REINFORCE(env.observation_space, env.action_space, config=config)\n",
    "\n",
    "avg_loss, trials = 0.0, 10\n",
    "for j in range(trials):\n",
    "    n_episodes = 100\n",
    "    _ = utils.runner.evaluate(agent, env, n_episodes, render=False)\n",
    "    loss = agent._loss_fn(agent.memory.sample())\n",
    "    avg_loss += loss\n",
    "\n",
    "avg_loss /= trials\n",
    "    \n",
    "assert 350.0 <= avg_loss <= 450.0, f\"Você obteve avg_loss = {avg_loss}\"\n",
    "print(\">> Sua implementação está provavelmente correta... Parabéns! :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Treinamento do agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "config = {\n",
    "    # policy net\n",
    "    \"hidden_layers\": [64, 64],\n",
    "    \"activation\": \"relu\",\n",
    "\n",
    "    # optimization\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"learning_rate\": 1e-3,\n",
    "\n",
    "    # training\n",
    "    \"train_batch_size\": 3000,\n",
    "}\n",
    "\n",
    "agent = REINFORCE(env.observation_space, env.action_space, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 100\n",
    "timesteps, total_rewards, avg_total_rewards = utils.runner.evaluate(agent, env, n_episodes, render=False)\n",
    "plot_action_distribution(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 3\n",
    "_ = utils.runner.evaluate(agent, env, n_episodes, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = 1_000_000\n",
    "timesteps, losses, grads, total_rewards, avg_total_rewards = utils.runner.train(agent, env, total_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_returns(timesteps, total_rewards, avg_total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradients(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 100\n",
    "timesteps, total_rewards, avg_total_rewards = utils.runner.evaluate(agent, env, n_episodes, render=False)\n",
    "plot_action_distribution(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 5\n",
    "_ = utils.runner.evaluate(agent, env, n_episodes, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experimentos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(trial, env_id, config, total_timesteps):\n",
    "    env = gym.make(env_id)\n",
    "    agent = REINFORCE(env.observation_space, env.action_space, config)\n",
    "    timesteps, losses, _, total_rewards, avg_total_rewards = utils.runner.train(agent, env, total_timesteps, verbose=True)\n",
    "    return timesteps, losses, total_rewards, avg_total_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 3\n",
    "total_timesteps = 1_000_000\n",
    "\n",
    "env_id = \"CartPole-v0\"\n",
    "base_config = {\n",
    "        # policy net\n",
    "    \"hidden_layers\": [64, 64],\n",
    "    \"activation\": \"relu\",\n",
    "\n",
    "    # optimization\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"learning_rate\": None,\n",
    "\n",
    "    # training\n",
    "    \"train_batch_size\": None,\n",
    "}\n",
    "\n",
    "learning_rates = [1e-2, 1e-4]\n",
    "batch_sizes = [1000, 10000]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:        \n",
    "        experiment_config = {\"learning_rate\": lr, \"train_batch_size\": bs}\n",
    "        config = {**base_config, **experiment_config}\n",
    "        \n",
    "        print(f\">> Experiment: {experiment_config}\")\n",
    "        results = []\n",
    "        for trial in range(n_trials):\n",
    "            print(f\"Starting trial {trial+1} ...\")\n",
    "            results.append(run_experiment(trial, env_id, config, total_timesteps))\n",
    "        \n",
    "        plot_experiments(results, experiment_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
