{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 2 - Parte Prática - Policy Gradients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "Nesse segundo notebook vamos aprender como melhorar o desempenho de um agente ao longo do tempo com base em experiências. Para isso, representaremos a política $\\pi_\\theta$ do agente por meio de uma rede neural de parâmetros $\\theta$.\n",
    "\n",
    "<img src=\"img/policy-net-agent-env-loop.png\" alt=\"Agent-Env Loop\" style=\"width: 550px;\"/>\n",
    "\n",
    "A abordagem que iremos estudar é conhecida por **Otimização de Política** baseada em gradientes (outro nome comum encontrado na literatura é Busca de Política (*Policy Search*)). De maneira geral, essa classe de algoritmos de RL é denominada como **Policy Gradients**.\n",
    "\n",
    "Começaremos a explorar Policy Gradients nesse notebook com uma implementação básica. A fim de desenvolver gradualmente seus principais componentes e entender o papel de cada um deles, iremos de forma didática construir o algoritmo **REINFORCE**.\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left [ \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(\\mathbf{a}_t|\\mathbf{s}_t)  \\sum_{t'=0}^{T-1} r_{t'+1} \\right ]\n",
    "$$\n",
    "\n",
    "\n",
    "### Objetivos:\n",
    "\n",
    "- Entender a abordagem de otimização de políticas como busca no espaço de parâmetros da política\n",
    "- Implementar um primeiro agente baseado no algoritmo REINFORCE\n",
    "- Familiarizar-se com a API básica de construção de modelos (i.e., redes neurais) em Keras\n",
    "- Familiarizar-se com métodos de Deep Learning usando TensorFlow 2.X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "> **Atenção:** não se esqueça de executar todos os `imports` necessários antes prosseguir com o tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "from utils.agent import RLAgent\n",
    "from utils.memory import OnPolicyReplay\n",
    "import utils.runner\n",
    "from utils.viz import *\n",
    "\n",
    "# sanity check\n",
    "assert tf.__version__ == '2.1.0'\n",
    "assert tf.executing_eagerly()\n",
    "\n",
    "tf.get_logger().setLevel('ERROR') # ignore TensorFlow warnings\n",
    "gym.logger.set_level(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Redes Neurais em TensorFlow e Keras\n",
    "\n",
    "Antes de começarmos propriamente com a implementação do algoritmo REINFORCE visto em aula. Iremos introduzir em linhas gerais a biblioteca de construção de modelos (i.e., redes neurai artificiais) do Keras no TensorFlow.\n",
    "\n",
    "A grosso modo, a API do Keras é uma camada de *software* implementada em cima das funções, classes e métodos dos pacotes de Deep Learning com o objetivo de facilitar a implementação, treinameto e avaliação de redes neurais. Maiores detalhes sobre a especificação do Keras podem ser acessadas em [https://keras.io/](https://keras.io/). A documentação específica da implementação do Keras para TensorFlow se encontra em [https://www.tensorflow.org/guide/keras](https://www.tensorflow.org/guide/keras).\n",
    "\n",
    "Note que a biblioteca do Keras é bastante extensa e flexível, de forma que nesse tutorial nos restringiremos a apresentar na medida do necessário os conceitos básicos necessários para trabalhar com redes neurais no contexto de Aprendizado por Reforço, e em particular, com foco nas primitivas necessárias para se implementar Policy Gradients.\n",
    "\n",
    "> **Observação**: para aqueles interessados em uma introdução mais detalhada, sugerimos seguir após a aula os tutoriais do Tensorflow (e.g., https://www.tensorflow.org/tutorials/quickstart/beginner).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Camadas e Modelos Sequenciais\n",
    "\n",
    "A rede neural mais simples que podemos construir é conhecida por *[Multi-Layer Perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron)* (MLP). Também conhecida por *Feedforward* ou *Fully-connected Network*.\n",
    "\n",
    "Uma MLP é definida pela composição de camadas de transformações afins, $h: \\mathbb{R}^q \\rightarrow \\mathbb{R}^p$, seguidas por uma função não-linear (e.g., *activation*):\n",
    "$\n",
    "h(\\mathbf{x}) = \\text{activation}(\\mathbf{W}_{p \\times q} \\cdot \\mathbf{x}_{q \\times 1} + \\mathbf{b}_{p \\times 1})\n",
    "$ .\n",
    "\n",
    "Recentemente, tem se tornado bastante comum o uso da função de ativação ReLU (*Rectified Linear Unit*):\n",
    "$\n",
    "\\text{ReLU}(x) = max(0, x)\n",
    "$ .\n",
    "\n",
    "> **Observação**: para esse curso não será necessário se aprofundar muito em redes neurais ou *Deep Learning*, no entanto, sugerimos como material auxiliar os excelentes vídeos explicativos sobre Redes Neurais do *3BLUE1BROWN* (https://www.3blue1brown.com/neural-networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Keras podemos utilizar a classe `tf.keras.layers.Dense` para construir a função $h(\\cdot)$ e a classe `tf.keras.Sequential` para encadear essas transformações (comumente chamadas de *camadas*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, input_shape=(5,), activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(3, activation=\"relu\"),\n",
    "])\n",
    "\n",
    "tf.keras.utils.plot_model(mlp, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(size=(5,))\n",
    "y = mlp(x[None,:])\n",
    "\n",
    "print(f\">> mlp(x) transforma vetores `x` de tamanho {x.shape[-1]} em vetores `y` de tamanho {y.shape[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É importante observar que camadas e modelos em Keras esperam receber vetores em *batch*. Dessa forma, é possível processar conjuntos de vetores em paralelo; o que torna a computação consideravelmente mais eficiente.\n",
    "\n",
    "No exemplo anterior, a variável `x = np.random.normal(size=(5,))` representa um único vetor de tamanho 5. Dessa forma, é necessário adicionar uma dimensão extra através do truque de manipulação de tensores `x[None,:]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(size=(5,))\n",
    "assert x.shape == (5,)\n",
    "assert x[None,:].shape == (1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para processar vetores em *batch*, o Keras assume que a primeira dimensão do vetor corresponde ao `batch_size`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "x = np.random.normal(size=(batch_size, 5))\n",
    "y = mlp(x)\n",
    "\n",
    "print(f\">> mlp(x) transformou {x.shape[0]} vetores `x` de tamanho {x.shape[-1]} em {y.shape[0]} vetores `y` de tamanho {y.shape[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Camadas Probabilísticas\n",
    "\n",
    "Em Aprendizado por Reforço é muitas vezes necessário representar distribuições (i.e., variáveis aleatórias). Por exemplo, na próxima seção utilizaremos uma distribuições para representar política estocásticas definida para um espaço discreto de ações.\n",
    "\n",
    "Em TensorFlow 2.X é fácil representar as principais distribuições através da biblioteca `tensorflow_probability` (https://www.tensorflow.org/probability).\n",
    "\n",
    "> **Observação**: para os interessados em modelos probabilísticos e inferência, recomendamos revisar após a aula o tutorial sobre regressão usando o TensorFlow Probability disponível em https://blog.tensorflow.org/2019/03/regression-with-probabilistic-layers-in.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esse tutorial precisaremos representar **distribuições categóricas** (https://en.wikipedia.org/wiki/Categorical_distribution) de forma a amostrar valores discretos ou classes (representadas por números inteiros) de acordo com um vetor de probabilidades.\n",
    "\n",
    "Lembre-se que para uma variável $x \\in \\{1, 2, \\cdots, K\\}$ e um vetor de probabilidades $[p_1, p_2, \\cdots, p_K]$ (i.e., $p_i \\in [0, 1], \\sum_{i=1}^K p_i = 1$), temos que:\n",
    "\n",
    "$$\n",
    "x \\sim \\text{Categorical}([p_1, p_2, \\cdots, p_K]) \\Leftrightarrow P(x = i) = p_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No TensorFlow podemos adicionar um camada probabilística através da classe `tfp.layers.DistributionLambda`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, input_shape=(5,), activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(4),\n",
    "    tfp.layers.DistributionLambda(lambda t: tfd.Categorical(logits=t))\n",
    "])\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se invocarmos a rede neural implementada pela variável `model` em um conjunto de vetores de entrada `inputs`, observamos que a saída corresponde a um objeto do tipo `tfp.distributions.Categorical` e não a valores da variável aleatória:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.random.normal(size=(8, 5))\n",
    "\n",
    "dist = model(inputs)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Atenção**: observe como o modelo representa uma distribuição parametrizada (e.g., os parâmetros de uma categórica são conhecidos por`logits`). As primeiras camadas do tipo `Dense` processam os vetores de entrada gerando um conjunto de vetores de *logits* usados para definir a distribuição condicional categórica. Em outras palavras, para cada vetor de entrada temos uma distribuição categórica (por isso dizemos que o modelo representa uma distribuição condicional, isto é, condicionada no vetor de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para amostrar realizações da variável aleatória dada pela distribuição categórica, utilizamos o método `sample()` (disponível para todas as distribuições definidas no TensorFlow Probability):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dist.sample()\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Atenção**: note que embora a entrada do `model` seja um array n-dimensional no NumPy, uma amostra da distribuição é um objeto `Tensor` do TensorFlow. Você pode usar o método `numpy()` para transformar `tf.Tensor` em um `np.ndarray`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outro método importante para se familiarizar é `log_prob`. Dada uma amostra ou vetores de amostra, o método `log_prob` devolve o logaritmo da probabilidade de se ter gerado a amostra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_log_prob = dist.log_prob(sample)\n",
    "print(sample_log_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Atenção**: o método `log_prob` será fundamental na implementação dos algoritmos de *Policy Gradientes* nessse curso. Tenha certeza que você compreendeu seu funcionamento em termos de entradas e saídas da função."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementando políticas estocásticas em tf.Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse tutorial iremos trabalhar com problemas do OpenAI gym que são definidos para um espaço discreto de ações. Dessa forma, precisaremos representar políticas estocásticas  $\\pi_\\theta(\\mathbf{a}_t | \\mathbf{s}_t)$ para ações discretas. Para isso, utilizaremos redes neurais construídas via Keras.\n",
    "\n",
    "<img src=\"img/policy-net.png\" alt=\"Agent-Env Loop\" style=\"width: 550px;\"/>\n",
    "\n",
    "Para não perdermos tempo com detalhes de implementação, disponibilizamos a função `build_discrete_policy` que recebe as especificações dos espaços de observação e ação do ambiente e os hiper-parâmetros que definem a arquitetura da rede neural:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discrete_policy(obs_space, action_space, hidden_layers, activation=\"relu\"):\n",
    "    Input = tf.keras.Input\n",
    "    Dense = tf.keras.layers.Dense\n",
    "    DistributionLambda = tfp.layers.DistributionLambda\n",
    "    Categorical = tfd.Categorical\n",
    "    \n",
    "    policy_net_layers = []\n",
    "\n",
    "    policy_net_layers.append(Input(shape=obs_space.shape, name=\"State\"))\n",
    "\n",
    "    for i, units in enumerate(hidden_layers):\n",
    "        policy_net_layers.append(Dense(units=units, activation=activation, name=f\"Hidden{i+1}\"))\n",
    "    \n",
    "    policy_net_layers.append(Dense(units=action_space.n, name=\"Logits\"))\n",
    "    policy_net_layers.append(DistributionLambda(lambda t: Categorical(logits=t), name=\"Action_Distribution_Categorical\"))\n",
    "                                     \n",
    "    return tf.keras.Sequential(policy_net_layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute o código abaixo para criar um modelo com 2 camadas de 64 unidades utilizando o `relu` como função de ativação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "hidden_layers = [64, 64]\n",
    "activation = \"relu\"\n",
    "\n",
    "policy = build_discrete_policy(env.observation_space, env.action_space, hidden_layers, activation)\n",
    "\n",
    "obs = env.observation_space.sample()\n",
    "action_dist = policy(obs[None,:])\n",
    "\n",
    "action = action_dist.sample().numpy()\n",
    "assert action[0] in env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode inspecionar visualmente a arquitetura da política (e de qualquer modelo no Keras) através da função `tf.keras.utils.plot_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(policy, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Agente REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez que já nos famialiarizamos com a construção de modelos via Keras e com as particularidades de uma política estocástica implementada como modelo probabilístico (via distribuição categórica), podemos começar a implementar a versão inicial do algoritmo *REINFORCE*. Lembre-se que, pela teoria desenvolvida em aula, o gradiente utilizado para otimizar a política é dado por:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left [ \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(\\mathbf{a}_t|\\mathbf{s}_t) \\hat{R}(\\tau)\\right ]~,\n",
    "$$\n",
    "onde $\\hat{R}(\\tau) = \\sum_{t=0}^{T-1} r_{t+1}$ é retorno de uma trajetória simulada no ambiente\n",
    "$\n",
    "\\tau = (\\mathbf{s}_0, \\mathbf{a}_0, r_1, \\mathbf{s}_1, \\mathbf{a}_1, r_2, \\cdots, \\mathbf{s}_{T-1}, \\mathbf{a}_{T-1}, r_{T}, s_{T}) \\sim \\pi_\\theta.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como não é possível calcular analiticamente esse valor esperado no caso geral, estimaremos via **Monte-Carlo** o gradiente da política:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{k=1}^K \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(\\mathbf{a}_t^{(k)}|\\mathbf{s}_t^{(k)}) \\hat{R}(\\tau^{(k)})~,\n",
    "$$\n",
    "\n",
    "onde o índice $k$ denota vetores da $k$-ésima trajetória amostrada. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Surrogate Loss e diferenciação automática\n",
    "\n",
    "Para usar a diferenciação automática no TensorFlow (i.e., computar gradientes via `tape.gradient`) é necessário definir uma função objetivo. No contexto de aprendizado de máquina no geral, essa função objetivo é tipicamente chamada de *loss* e a maioria dos pacotes de *Deep Learning* tenta **minimizar essa função**.\n",
    "\n",
    "Dessa forma, como estamos querendo maximizar o desempenho do agente denotado por $J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} [R(\\tau)]$ deveremos minimizar o negativo de $J(\\theta)$. Para o Policy Gradients a função de *loss* que vamos utilizar é dada por:\n",
    "\n",
    "$$\n",
    "\\hat{J}(\\theta) = \\frac{1}{N} \\sum_{k=1}^K \\sum_{t=0}^{T-1}  \\log \\pi_\\theta(\\mathbf{a}_t^{(k)}|\\mathbf{s}_t^{(k)}) \\hat{R}(\\tau^{(k)})~.\n",
    "$$\n",
    "\n",
    "> **IMPORTANTE**: note que em geral $\\hat{J}(\\theta) \\neq J(\\theta)$, no entanto, $\\nabla_\\theta \\hat{J}(\\theta) \\approx \\nabla_\\theta J(\\theta)$. Essa é a razão pela qual $\\hat{J}$ é chamada de *surrogate loss*; *surrogate* em português significa \"substituto\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**<font color=\"red\">EXERCÍCIO-PROGRAMA:</font>**\n",
    "\n",
    "Nesse exercício você deverá codificar o método `_loss_fn` da classe `REINFORCE` que será reponsável por implementar a função de *loss* do *Policy Gradient*.\n",
    "\n",
    "> **Nota 1**: observe que cada `batch` corresponde a um dicionário de listas, onde cada lista corresponde a um conjunto de episódios. Por exemplo, `batch[\"states\"][k]` devolve os estados do k-ésimo episódio amostrado.\n",
    "\n",
    "> **Nota 2**: No NumPy e no TensorFlow você pode trabalhar com operações binárias \"vetorizadas\", isto é, você pode por exempo multiplicar 2 conjuntos de vetores `x` e`y` de mesmo tamanho (e.g., `x.shape == y.shape == (N, dim)`, onde `N` representa o número de vetores e `dim` o tamanho do vetor) simplesmente executando `x * y`. Essa técnica é conhecida como *[broadcasting](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)* de vetores (ou array n-dimensionais no caso mais geral).\n",
    "\n",
    "> **Nota 3**: pode ser útil utilizar as funções de agregação `np.sum` e `tf.reduce_sum`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE(RLAgent):\n",
    "    \"\"\"\n",
    "    Classe que implementa os componentes de um agente que aprende pelo REINFORCE.\n",
    "\n",
    "    Args:\n",
    "        obs_space:     especificação do espaço de observações do ambiente.\n",
    "        action_space:  especificação do espaço de ações do ambiente.\n",
    "        config (dict): (opcional) configurações de hiper-parâmetros.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, obs_space, action_space, config=None):\n",
    "        super(REINFORCE, self).__init__(obs_space, action_space, config)\n",
    "        \n",
    "        self.memory = OnPolicyReplay()\n",
    "        self.policy = build_discrete_policy(self.obs_space, self.action_space, config[\"hidden_layers\"], config[\"activation\"])\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=config[\"learning_rate\"])\n",
    "\n",
    "    def act(self, obs):\n",
    "        \"\"\"\n",
    "        Escolhe uma ação para ser tomada dada uma observação do ambiente.\n",
    "        \n",
    "        Args: \n",
    "            obs: observação do ambiente.\n",
    "        \n",
    "        Return:\n",
    "            action: ação válida dentro do espaço de ações.\n",
    "        \"\"\"\n",
    "\n",
    "        return self._act(obs).numpy()\n",
    "    \n",
    "    @tf.function\n",
    "    def _act(self, obs):\n",
    "        action_dist = self.policy(obs[None,:])\n",
    "        return action_dist.sample()[0]\n",
    "\n",
    "    def observe(self, obs, action, reward, next_obs, done):\n",
    "        \"\"\"\n",
    "        Registra na memória do agente uma transição do ambiente.\n",
    "\n",
    "        Args:\n",
    "            obs:            observação do ambiente antes da execução da ação.\n",
    "            action:         ação escolhida pelo agente.\n",
    "            reward (float): escalar indicando a recompensa obtida após a execução da ação.\n",
    "            next_obs:       nova observação recebida do ambiente após a execução da ação.\n",
    "            done (bool):    True se a nova observação corresponde a um estado terminal, False caso contrário.\n",
    "\n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.memory.update(obs, action, reward, next_obs, done)\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Método de treinamento do agente. A partir das experiências de sua memória,\n",
    "        o agente aprende um novo comportamento.\n",
    "\n",
    "        Args: \n",
    "            None\n",
    "\n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"     \n",
    "        if self.memory.batch_size < self.config[\"train_batch_size\"]:\n",
    "            return\n",
    "        \n",
    "        batch = self.memory.sample()\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._loss_fn(batch)\n",
    "            gradients = tape.gradient(loss, self.policy.trainable_weights)\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.policy.trainable_weights))\n",
    "        \n",
    "        grad_norm = [(w.name, tf.norm(grad).numpy()) for w, grad in zip(self.policy.trainable_weights, gradients)]\n",
    "      \n",
    "        return loss.numpy(), grad_norm\n",
    "\n",
    "    def _loss_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Calcula a função loss do policy gradients para um `batch` de trajetórias/episódios.\n",
    "        \n",
    "        Um `batch` agrega listas de arrays n-dimensionais. Cada lista (e.g., batch[\"states\"],\n",
    "        batch[\"actions\"], batch[\"rewards\"]) tem o tamanho do número de episódios. Por exemplo,\n",
    "        batch[\"states\"][k] devolve um array n-dimensional para o k-ésimo episódio. Este array\n",
    "        tem como primeira dimensão o número de timesteps do k-ésimo episódio.\n",
    "\n",
    "        Args:\n",
    "            batch (Dict[str, List[np.ndarray]]): dicionário para acesso às listas de estados, ações e recompensas. \n",
    "        \n",
    "        Return:\n",
    "            loss (tf.Tensor): média sobre os episódios do surrogate loss function.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        states, actions, rewards = batch[\"states\"], batch[\"actions\"], batch[\"rewards\"]\n",
    "        n_episodes = len(states)\n",
    "\n",
    "        loss = 0.0\n",
    "\n",
    "        # SEU CÓDIGO AQUI ===================================\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # ===================================================\n",
    "\n",
    "        assert loss.shape == (), \"loss deve ser um escalar\"\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para testar a sua implementação, rode o código abaixo. Para os parâmetros definidos abaixo no dicionário de `config`, você deve obter um `avg_loss` entre 300.0 e 600.0. Note que estamos usando simulação de Monte-Carlo então o resultado pode variar um pouco.\n",
    "\n",
    "> **IMPORTANTE**: caso a sua implementação não esteja passando no teste abaixo (em várias tentativas consecutivas) e você achar que sua implementação está correta, chame o monitor ou um dos instrutores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "config = {\n",
    "    # policy net\n",
    "    \"hidden_layers\": [64, 64],\n",
    "    \"activation\": \"relu\",\n",
    "\n",
    "    # optimization\n",
    "    \"learning_rate\": 5e-4,\n",
    "\n",
    "    # training\n",
    "    \"train_batch_size\": 3000,\n",
    "}\n",
    "\n",
    "agent = REINFORCE(env.observation_space, env.action_space, config=config)\n",
    "\n",
    "avg_loss, trials = 0.0, 10\n",
    "for j in range(trials):\n",
    "    n_episodes = 100\n",
    "    _ = utils.runner.evaluate(agent, env, n_episodes, render=False)\n",
    "    loss = agent._loss_fn(agent.memory.sample())\n",
    "    avg_loss += loss\n",
    "\n",
    "avg_loss /= trials\n",
    "    \n",
    "print(f\"Você obteve avg_loss = {avg_loss}.\")\n",
    "assert 300.0 <= avg_loss <= 600.0\n",
    "print(\"\\n>> Sua implementação está provavelmente correta... Parabéns! :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Treinamento do agente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para treinar o agente seguiremos o algoritmo visto em aula.\n",
    "\n",
    "<img src=\"img/reinforce.png\" alt=\"Agent-Env Loop\" style=\"width: 750px;\"/>\n",
    "\n",
    "No entanto, para agilizar o tutorial e garantir que todos tenham a mesma implementação do ciclo do agente-ambiente, disponibilizados nos métodos `utils.runner.train` e `utils.runner.evaluate` que se encarregam em facilitar a coleta de experiências pelo agente em um ambiente.\n",
    "\n",
    "A função `utils.runner.evaluate` simula um dado número de episódios completos no ambiente a fim de avaliar o retorno médio do agente. Por outro lado, a função `utils.runner.train` coleta o certo número de passos no ambiente e chama o método `learn` do agente que recupera de sua memória um `batch` de experiências para estimar o *policy gradient*.\n",
    "\n",
    "Nessa seção, você não precisará implementar nada para treinar o agente. Mas vai rodar um primeiro experimento mais demorado para avaliar o desempenho do agente.\n",
    "\n",
    "> **Observação**: embora não seja necessário, caso tenha curiosidade em saber mais sobre essas funções você pode acessar o código via o *File Browser* na lateral do Jupyter Lab na pasta `utils/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Otimização & Gradiente Ascendente\n",
    "\n",
    "\n",
    "<img src=\"img/sgd.png\" alt=\"Agent-Env Loop\" style=\"width: 300px;\"/>\n",
    "\n",
    "Para treinar o agente com o algoritmo do *Policy Gradients* iremos usar um método de otimização baseado na técnica de **[Gradiente Ascendente](https://en.wikipedia.org/wiki/Gradient_descent)**:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)\n",
    "$$\n",
    "\n",
    "Se você prestou atenção na definição da classe `REINFORCE` deve ter notado que:\n",
    "1. No construtor `__init__` da classe é instanciado um otimizador `self.optimizer = tf.keras.optimizers.Adam(learning_rate=...)`; e\n",
    "2. No método `learn`, o otimizador é chamado para aplicar os gradients `self.optimizer.apply_gradients(...`).\n",
    "\n",
    "Nessa parte prática optamos por usar um dos otimizadores mais utilizados em aprendizado por máquina, e.g., [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam).\n",
    "\n",
    "> **IMPORTANTE**: não precisaremos implementar o algoritmo de gradiente ascendente, isto é, o otimizador irá se encarregar disso para nós!\n",
    "\n",
    "> **Observação**: a bilioteca do TensorFlow conta com inúmeros outros otimizadores (e.g., https://www.tensorflow.org/api_docs/python/tf/keras/optimizers).Uma apresentação mais detalha desses otimizadores está fora do escopo desse curso. Para aqueles interessados em aprender sobre otimizadores, sugerimos o tutorial disponível em https://ruder.io/optimizing-gradient-descent/.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Avaliação do agente (antes do treinamento) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinicialize o agente executando o código abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "config = {\n",
    "    # policy net\n",
    "    \"hidden_layers\": [64, 64],\n",
    "    \"activation\": \"relu\",\n",
    "\n",
    "    # optimization\n",
    "    \"learning_rate\": 5e-4,\n",
    "\n",
    "    # training\n",
    "    \"train_batch_size\": 3000,\n",
    "}\n",
    "\n",
    "agent = REINFORCE(env.observation_space, env.action_space, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de treinarmos o agente, é importante ter alguma referência sobre seu desempenho inicial. Execute o código abaixo para coletar `n_episodes` e obter o retorno médio por episódio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1000\n",
    "_, total_rewards, _ = utils.runner.evaluate(agent, env, n_episodes, render=False)\n",
    "batch = agent.memory.sample()\n",
    "\n",
    "print(f\"\\n>> O retorno médio obtido em {n_episodes} episódios foi {np.mean(total_rewards)}\")\n",
    "print(f\">> Note que para o problema do CartPole-v0 ser considerado resolvido seria necessário obter pelo menos {env.spec.reward_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outro ponto importante para se ter em mente antes do treinamento é a inicialização da política. É fundamental iniciar o treinamento com uma política que aproxima uma distribuição uniforme. Dessa forma, garantimos que o agente começará a interagir com o ambiente sem estar enviesado em suas escolhas das ações.\n",
    "\n",
    "Execute o comando abaixo para avaliar as distribuições das ações para algumas observações que você acabou de coletar. Verifique que independentemente da observação, o agente não tem nenhuma predileção muito forte por certa ação. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_action_distribution(agent, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, execute o comando abaixo para inspecionar visualmente alguns episódios. Note que como o agente ainda não aprendeu nada, **cada episódio falha muito rapidamente** (e.g., no caso do *CartPole* isso quer dizer que o carrinho não consegue equilibrar o mastro e por isso este cai muito rapidamente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 10\n",
    "_ = utils.runner.evaluate(agent, env, n_episodes, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Aprendendo melhores políticas\n",
    "\n",
    "Finalmente vamos treinar o agente. Note que para 1 milhão de passos de interação no ambiente a simulação deve demorar alguns poucos minutos.\n",
    "\n",
    "> **Observação**: lembra quando dissemos na aula que aprendizado por reforço precisa de muitos dados? Para um problema relativamente simples como o *CartPole* em geral é necessário dezenas ou centenas de milhares para um algoritmo do estado-da-arte. Para o REINFORCE, que é um dos algoritmos mais básicos, precisaremos de mais experiências. Nas próximas aulas vamos estudar técnicas mais avançadas para reduzir o número de amostras necessárias para obtermos o mesmo desempenho.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = 1_000_000\n",
    "timesteps, losses, grads, total_rewards, avg_total_rewards = utils.runner.train(agent, env, total_timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Visualizando a curva de aprendizado e outras métricas do treinamento "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talvez a métrica mais importante para se avaliar ao longo do treinamento de um agente de RL seja o retorno esperado dos episódios. Execute o código abaixo para visualizar o retorno médio por *batch* de trajetórias e uma média móvel sobre os últimos 100 episódios.\n",
    "\n",
    "Lembre-se que para o *CartPole* ser considerado como resolvido é necessário obter pelo menos 195.0 de retorno médio nos últimos 100 episódios. \n",
    "\n",
    "> **Atenção**: note o quão \"ruidoso\" é o treinamento do REINFORCE na versão básica que acabamos de implementar. Às vezes o retorno aumenta, logo em seguida diminui, e depois melhora novamente. Mesmo suavizando a curva dos retornos através uma média móvel, ainda é possível visualizar que o treinamento está longe de ser estável. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_returns(timesteps, total_rewards, avg_total_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute o código abaixo para visualizar o *policy gradient loss*. Lembre-se que usamos essa quantidade para estimar o gradiente da política e não para avaliar o desempenho do agente. Embora para o caso do REINFORCE  a *loss* pareça estar correlacionada com o retorno médio, isso não ocorre no geral uma vez que para algoritmos mais avançados termos auxiliares aparecerão do cálculo da *loss function*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também pode ser útil para *debuggar* possíveis problemas ter alguma ideia da norma $\\mathcal{L}_2$ dos gradientes, $||\\nabla_\\theta J(\\theta)||_2$, ao longo do treinamento.\n",
    "\n",
    "Em particular, é importante garantir que a magnitude da norma dos gradientes não \"exploda\".\n",
    "\n",
    "Se você implementou tudo corretamente (e não alterou nada na arquitetura do modelo da política), a maioria dos valores das normas devem estar entre 500 e 1500, com alguns picos até 4000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_norms(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após o treinamento, compare o novo desempenho do agente em termos de retorno médio, distribuições das ações e comportamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 100\n",
    "timesteps, total_rewards, avg_total_rewards = utils.runner.evaluate(agent, env, n_episodes, render=False)\n",
    "batch = agent.memory.sample()\n",
    "\n",
    "print(f\"\\n>> O retorno médio obtido em {n_episodes} episódios foi {np.mean(total_rewards)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a nova distribuição de ações:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_action_distribution(agent, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que se tudo correu conforme o esperado o agente aprendeu a equilibrar o mastro movendo o carrinho de acordo. Ainda há como melhorar um pouco, mas já é possível verificar que o agente aprendeu algo bastante razoável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 10\n",
    "_ = utils.runner.evaluate(agent, env, n_episodes, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**<font color=\"red\">QUESTÕES:</font>**\n",
    "\n",
    "1. Como você compara as distribuições das ações (isto é, a política estocástica) antes e depois do treinamento? O que você diria sobre a escolha de ações pelo agente no início e no final do treinamento?\n",
    "2. Como você justificaria o ruído (ou em outras palavras a alta variabilidade) nos retornos dos episódios obtidos durante o treinamento? Como você relaciona o *policy gradient loss* e a norma dos gradientes com os retornos obtidos?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bônus - Experimentos\n",
    "\n",
    "Nessa seção opcional, vamos tentar entender o efeito na curva de treinamento de alguns hiper-parâmetros do algoritmo REINFORCE. Em particular, iremos variar a taxa de aprendizado (e.g., *learning rate*) e o tamanho do *batch* de treinamento.\n",
    "\n",
    "Execute o código abaixo para rodar 4 experimentos. Note que irá demorar certa de 15 e 30 minutos dependendo da máquina. A medida que os resultados forem sendo visualizados, tente formular hipóteses sobre o efeito do hiper-parâmetro. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(trial, env_id, config, total_timesteps):\n",
    "    env = gym.make(env_id)\n",
    "    agent = REINFORCE(env.observation_space, env.action_space, config)\n",
    "    timesteps, losses, _, total_rewards, avg_total_rewards = utils.runner.train(agent, env, total_timesteps, verbose=True)\n",
    "    return timesteps, losses, total_rewards, avg_total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 3\n",
    "total_timesteps = 1_000_000\n",
    "\n",
    "env_id = \"CartPole-v0\"\n",
    "base_config = {\n",
    "        # policy net\n",
    "    \"hidden_layers\": [64, 64],\n",
    "    \"activation\": \"relu\",\n",
    "\n",
    "    # optimization\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"learning_rate\": None,\n",
    "\n",
    "    # training\n",
    "    \"train_batch_size\": None,\n",
    "}\n",
    "\n",
    "learning_rates = [1e-2, 1e-4]\n",
    "batch_sizes = [1000, 10000]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:        \n",
    "        experiment_config = {\"learning_rate\": lr, \"train_batch_size\": bs}\n",
    "        config = {**base_config, **experiment_config}\n",
    "        \n",
    "        print(f\">> Experiment: {experiment_config}\")\n",
    "        results = []\n",
    "        for trial in range(n_trials):\n",
    "            print(f\"Starting trial {trial+1} ...\")\n",
    "            results.append(run_experiment(trial, env_id, config, total_timesteps))\n",
    "        \n",
    "        plot_experiments(results, experiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**<font color=\"red\">QUESTÕES:</font>**\n",
    "\n",
    "1. Fixando o `batch_size`, como você compararia os resultados obtidos para cada valor de `learning_rate` em termos do desempenho final do agente e estabilidade do treinamento?\n",
    "2. Fixando o `learning_rate`, como você compararia os resultados obtidos para cada valor de `batch_size` em termos do desempenho final do agente e estabilidade do treinamento?\n",
    "3. Se você tivesse possibilidade (e paciência :)) de coletar 10 milhões de passos de interação com ambiente qual dentre as quatro configurações de hiper-parâmetros você escolheria? E se tivesse apenas 100.000 ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
