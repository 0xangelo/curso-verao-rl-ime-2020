{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 1 - Parte Prática - Agentes & Ambientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "Nesse primeiro notebook iremos aprender sobre a API do [OpenAI Gym](http://gym.openai.com/) e começaremos a implementar alguns componentes de um agente de RL.\n",
    "Ao final dessa parte prática teremos implementado o **ciclo de interação Agente-Ambiente** que nos permitirá futuramente treinar o agente e avaliar sua performance.\n",
    "\n",
    "<img src=\"img/agent-env-loop.png\" alt=\"Agent-Env Loop\" style=\"width: 450px;\"/>\n",
    "\n",
    "\n",
    "### Objetivos:\n",
    "\n",
    "- Relacionar os conceitos de Processos de Decisão Markovianos (MDPs) com os atributos e métodos de um ambiente definido com o OpenAI Gym;\n",
    "- Familiarizar-se com os componentes básicos de um agente de RL;\n",
    "- Implementar o ciclo de interação Agente-Ambiente; e\n",
    "- Implementar um primeiro agente aleatório e avaliar sua performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "> **Atenção:** não se esqueça de executar todos os `imports` necessários antes prosseguir com o tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import random\n",
    "\n",
    "import gym\n",
    "\n",
    "from utils.viz import plot_action_distribution, plot_episode_total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ambientes no OpenAI Gym\n",
    "\n",
    "Um ambiente no OpenAI Gym encapsula um simulador com o qual um agente pode interagir. Nesse simulador, a cada instante $t$ o agente deve escolhar uma ação $\\mathbf{a}_t \\in \\mathcal{A}$ a ser executada. Ao receber essa ação, o ambiente tem seu estado $\\mathbf{s}_t \\in \\mathcal{S}$ alterado para outro estado $\\mathbf{s}_{t+1} \\in \\mathcal{S}$ e devolve para o agente uma observação (que pode ou não corresponder ao estado) e uma recompensa/punição $r_{t+1} \\in \\mathbb{R}$.\n",
    "\n",
    "O pacote Gym conta com inúmeros [ambientes pré-programados](http://gym.openai.com/envs/) e prontos para serem usados para testar algoritmos de RL. Nessa parte prática, começaremos a explorar alguns ambientes mais simples a fim de nos familiarizarmos com os principais conceitos de modelagem da biblioteca Gym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para carregar um ambiente disponível no Gym basta chamar a função `gym.make` passando como argumento o identificador do ambiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Atenção**: antes de usar um ambiente do Gym com o qual você não está familiarizado pode ser útil ler a sua documentação online.\n",
    "> Para o `MountainCar-v0` acesse o link [http://gym.openai.com/envs/MountainCar-v0/](http://gym.openai.com/envs/MountainCar-v0/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Informações adicionais sobre o ambiente/simulador podem ser obtidas accessando o atributo `env.spec`. Embora não seja obrigatório, muitos ambientes do OpenAI Gym definem  metadados importantes relacionados à tarefa de RL. Em particular, `env.spec.max_episode_steps` define o número máximo de passos de decisão que um agente pode tomar em um episódio (i.e., o tamanho máximo de uma trajetória) e `env.spec.reward_threshold` define o valor mínimo de retorno (i.e., recompensa total) de um episódio para o qual a tarefa é considerada resolvida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MAX_EPISODE_STEPS = {env.spec.max_episode_steps}\")\n",
    "print(f\"REWARD THRESHOLD = {env.spec.reward_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Espaço de estados e ações\n",
    "\n",
    "Todo agente de RL deve conhecer quais ações pode tomar no ambiente e também quais as características das variáveis de observações que aquele ambiente lhe disponibiliza. Para acessar o espaço de estados e ações, um ambiente do gym disponibiliza os atributos `env.observation_space` e `env.action_space`, respectivamente.\n",
    "\n",
    "Note que essas informações serão importantes futuramente na definição das entradas e saídas das redes neurais artificiais que utilizaremos para representar a política $\\pi_\\theta$ do agente e também na criação de outros modelos.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo `observation_space` tem associado seu tipo numérico (e.g., int, float,...) e as dimensões de uma observação. Além disso, é possível saber se o valor das variáveis observação são limitadas ou não, e se forem limitadas qual o valor mínimo e máximo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_space = env.observation_space\n",
    "\n",
    "print(obs_space.dtype)\n",
    "print(obs_space.shape)\n",
    "print(obs_space.bounded_above, obs_space.low)\n",
    "print(obs_space.bounded_below, obs_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o `MountainCar-v0` note que uma observação correponde a um vetor de números reais de tamanho 2 (i.e., um par de valores em ponto flutuante). Note que a primeira componente do vetor é limitada entre -1.2 e 0.6, e a segunda componente entre -0.07 e 0.07. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogamente, todo `action_space` tem associado seu tipo numérico e as dimensões de uma ação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = env.action_space\n",
    "\n",
    "print(action_space.dtype)\n",
    "print(action_space.shape)\n",
    "print(action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que para o `MountainCar-v0` o agente deverá escolher ações discretas (i.e., representadas por números inteiros). Observe também que o `shape==()` indica que uma ação é dada por um único escalar (e não um vetor como no caso do `observation_space`. Para acessar o número de possíveis valores da ação basta acessar o atributo `action_space.n`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Atenção**: em algumas situações pode ser interessante ter acesso à amostras de observações e ações. Você pode usar os métodos `env.observation_space.sample()` e `env.action_space.sample()` para gerar aleatoriamente observações e ações direto do ambiente `env`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_samples = [env.observation_space.sample() for _ in range(3)]\n",
    "print(obs_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_samples = [env.action_space.sample() for _ in range(5)]\n",
    "print(action_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Interface do Gym: métodos reset, step, render e close\n",
    "\n",
    "Um objeto `env` do Gym fornece 4 métodos principais para interagir com o simulador: \n",
    "1. `reset` permite re-inicialiar o simulador para um de seus estados iniciais;\n",
    "2. `step` se encarrega de executar uma ação no ambiente;\n",
    "3. `render` visualiza graficamente o estado do agente; e\n",
    "4. `close` libera recursos utilizados na simulação (por exemplo fecha a janela de visualização)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    obs = env.reset()\n",
    "    print(f\"obs {i} = {obs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que a observação do estado inicial muda conforme o método `.reset()` é chamado. Isso se deve ao fato de que o estado inicial é definido como uma variável aleatória regida por uma distribuição inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "for i in range(5):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print(f\"transition {i} = ({action}, {obs}, {reward})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Atenção**: Você pode se familiarizar com os argumentos e retorno de ambos os métodos através de seus *docstrings* acessados via `??`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Agentes \n",
    "\n",
    "A fim de permitir a implementação do **ciclo de interação Agente-Ambiente**, um agente de RL deve ser capaz de escolher uma ação para cada observação recebida do ambiente e aprender (i.e., melhorar sua performance) a partir de suas experiências.\n",
    "\n",
    "Nesse contexto, na classe abstrata `Agent` definimos a interface geral de um agente de RL. \n",
    "\n",
    "> **Atenção**: Familiarize-se com essa classe; todos os agentes definidos nessa aula e nas próximas deverão especializar (i.e., derivar ou sub-classear) essa interface geral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent:\n",
    "    \"\"\"\n",
    "    Classe abstrata que define a interface básica de um agente RL.\n",
    "\n",
    "    Args:\n",
    "        obs_space:     especificação do espaço de observações do ambiente.\n",
    "        action_space:  especificação do espaço de ações do ambiente.\n",
    "        config (dict): (opcional) configurações de hiper-parâmetros.\n",
    "    \"\"\"\n",
    "    \n",
    "    __metaclass__ = abc.ABCMeta\n",
    "\n",
    "    def __init__(self, obs_space, action_space, config=None):\n",
    "        self.obs_space = obs_space\n",
    "        self.action_space = action_space\n",
    "        self.config = config\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def act(self, obs):\n",
    "        \"\"\"\n",
    "        Escolhe uma ação para ser tomada dada uma observação do ambiente.\n",
    "        \n",
    "        Args: \n",
    "            obs: observação do ambiente.\n",
    "        \n",
    "        Return:\n",
    "            action: ação válida dentro do espaço de ações.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def observe(self, obs, action, reward, next_obs, done):\n",
    "        \"\"\"\n",
    "        Registra na memória do agente uma transição do ambiente.\n",
    "\n",
    "        Args:\n",
    "            obs:            observação do ambiente antes da execução da ação.\n",
    "            action:         ação escolhida pelo agente.\n",
    "            reward (float): escalar indicando a recompensa obtida após a execução da ação.\n",
    "            next_obs:       nova observação recebida do ambiente após a execução da ação.\n",
    "            done (bool):    True se a nova observação corresponde a um estado terminal, False caso contrário.\n",
    "\n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Método de treinamento do agente. A partir das experiências de sua memória,\n",
    "        o agente aprende um novo comportamento.\n",
    "\n",
    "        Args: \n",
    "            None\n",
    "\n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"     \n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Definindo um agente aleatório\n",
    "\n",
    "Antes de finalmente definir o ciclo de interação agente-ambiente, vamos implementar um agente que escolhe ações aleatórias.\n",
    "\n",
    "O agente `RandomPolicy` tem seu comportamento definido por uma política estocástica dada por uma distribuição uniforme sobre as ações válidas:\n",
    "\n",
    "$$\n",
    "\\mathbf{a}_t \\sim \\pi(\\cdot|\\mathbf{s}) = \\mathcal{Uniform}(\\{ \\mathbf{a} : \\mathbf{a} \\in \\mathcal{A} \\})~.\n",
    "$$\n",
    "\n",
    "Note que nesse ponto do curso, a implementação do agente aleatório é basicamente ilustrativa. No entanto, como veremos nas aulas seguintes, um agente que implementa uma política aleatória tem duas importantes funções:\n",
    "1. servir de referência de performance final; e\n",
    "2. guiar a inicialização de agentes de RL.\n",
    "\n",
    "Em outras palavras, se um agente de RL após o treinamento não conseguir uma performance significativamente melhor do que aquela do agente aleatório, então muito provavelemente algo não está funcionando como deveria. Além disso, ao garantir que a inicialização de uma política induza um comportamente similar ao de um agente aleatório, não estaremos enviesando a exploração inicial do agente; o que poderia levar muito rapidamente para uma performance sub-ótima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy(RLAgent):\n",
    "    \"\"\"\n",
    "    Agente aleatório. Escolhe aleatoriamente uma ação independentemente\n",
    "    da observação recebida do ambiente.\n",
    "    \n",
    "    Args:\n",
    "        action_space:  especificação do espaço de ações do ambiente.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, observation_space, action_space, config=None):\n",
    "        super(RandomPolicy, self).__init__(observation_space, action_space, config)\n",
    "\n",
    "    def act(self, obs):\n",
    "        \"\"\"Retorna uma ação aleatória.\"\"\"\n",
    "        return self.action_space.sample()\n",
    "\n",
    "    def observe(self, obs, action, reward, next_obs, done):\n",
    "        \"\"\"Ignora transições; i.e., um agente aletório não armazena experiências na memória.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Um agente aletório não aprende; i.e., não melhora seu comportamento.\"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que para uma mesma observação o agente `RandomPolicy` retorna diferentes ações a cada chamada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RandomPolicy(env.observation_space, env.action_space)\n",
    "\n",
    "obs = env.reset()\n",
    "print([agent.act(obs) for _ in range(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_action_distribution(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ciclo de Interação Agente-Ambiente\n",
    "\n",
    "Após entender a API do OpenAI Gym e se familiarizar com a interface geral de um agente de RL, estamos pronto para programar o ciclo de interação Agente-Ambiente.\n",
    "\n",
    "Note que tanto o *treinamento* como a *avaliação de performance* de agentes de RL (baseados em simuladores) dependem da coleta de experiências a fim de estimar uma diferentes grandezas que são necessárias nos algoritmos de RL (e.g., retorno de episódios, gradientes de políticas, ...).\n",
    "\n",
    "Esse é o objetivo principal do ciclo de interação com o ambiente: permitir ao agente explorar o ambiente e coletar dados para seu aprendizado.\n",
    "\n",
    "> **Atenção**: Praticamente todos os pacotes de RL disponíveis implementam uma versão desse loop de interação. Dessa forma, independentemente se seu objetivo é desenvolver sua própria biblioteca de RL ou apenas re-utilizar código pré-existente, é importante entender os principais conceitos envolvidos!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa seção testaremos a política aleatória em uma outra versão do ambiente MountainCar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "agent = RandomPolicy(env.observation_space, env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**<font color=\"red\">EXERCÍCIO-PROGRAMA 1:</font>**\n",
    "\n",
    "Nesse exercício você deverá utilizar os métodos da API do Gym a fim de permitir que um agente de RL simule um episódio. Complete a função `sample_episode` com seu código e preste atenção para retornar as variáveis definidas na documentação. Caso necessário revise a <a href=\"/lab#1.2-Interface-do-Gym:-métodos-reset(),-step()-e-render()\" target=\"_self\">Seção 1.2</a>.\n",
    "\n",
    ">**Atenção**: Para visualizar o episódio não se esqueça de chamar `env.render()` durante a simulação da trajetória. Use a flag `render` em um *if-statement* do Python para dinamicamente habilitar a visualização. Uma vez que a parte gráfica consome muito tempo, é comum desabilitar a renderização do ambiente durante o treinamento e avaliação de um agente de RL. Além disso, chame `env.close()` ao final do ciclo para fechar a janela de simulação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_episode(agent, env, render=False):\n",
    "    \"\"\"\n",
    "    Simula um episódio completo de interação do agente com o ambiente.\n",
    "    \n",
    "    Args:\n",
    "        agent (RLAgent):       agente responsável por retornar ações.\n",
    "        env (gym.Environment): simulador de ambiente do OpenAI Gym.\n",
    "        render (bool):         (opcional) flag para habilitar a renderização do ambiente.\n",
    "        \n",
    "    Return:\n",
    "        (total_reward, episode_length): retorno obtido pelo agente no episódio e\n",
    "        número de passos de decisão realizados no episódio.\n",
    "    \"\"\"\n",
    "    total_reward = 0.0\n",
    "    episode_length = 0\n",
    "\n",
    "    # SEU CÓDIGO AQUI ===================================\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ===================================================\n",
    "\n",
    "    return total_reward, episode_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute o código abaixo para testar a sua implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward, episode_length = sample_episode(agent, env, render=True)\n",
    "print(f\"return = {total_reward:.4f}, passos de decisão = {episode_length}\\r\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A menos que você tenha tido sorte na simulação, você deve ter observado que o carro nunca chega próximo à linha de chegada. :(\n",
    "\n",
    "Não se preocupe vamos resolver isso nas próximas aulas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez implementado a função `sample_episode` você pode amostrar diferentes trajetórias com a função `run` definida abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(agent, env, num_episodes):\n",
    "    episode_returns, episode_lengths = [], []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        total_reward, episode_length = sample_episode(agent, env)\n",
    "    \n",
    "        episode_returns.append(total_reward)\n",
    "        episode_lengths.append(episode_length)\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"episode = {episode}, return = {total_reward:.4f}, length = {episode_length}\\r\", end=\"\")\n",
    "\n",
    "    return episode_returns, episode_lengths\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute o código abaixo para simular `NUM_EPISODES` trajetórias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 200\n",
    "\n",
    "episode_returns, episode_lengths = run(agent, env, NUM_EPISODES)\n",
    "\n",
    "plot_episode_total_rewards(episode_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Atenção**: se você obteve um pico de retorno com um valor positivo (i.e., *outlier*), execute novamente a simulação. Caso contrário, você deve ter obtido um retorno médio (vide linha vermelha) entre -34 e -33 para `NUM_EPISODES==200`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**<font color=\"red\">QUESTÕES:</font>**\n",
    "\n",
    "1. Qual a diferença entre o ambiente `MountainCar-v0` utilizado como exemplo na <a href=\"/lab#1.-Ambientes-no-OpenAI-Gym\" target=\"_self\">Seção 1</a> e o ambiente `MountainCarContinuous-v0` que você acabou de simular?\n",
    "2. Como você interpreta os gráficos acima `Episode Return` e `Episode Return (Histogram)` ? Como você explicaria essas variações ruidosas da recompensa total?\n",
    "3. Se você executar a simulação várias vezes, obterá resultados ligeiramente diferentes? Ao que se deve essa incerteza nos resultados?\n",
    "4. Você diria que o agente aleatório obteve uma boa performance? *Dica*: relacione os resultados obtidos com a especificação do ambiente `MountainCarContinuous-v0` (e.g., `env.spec`).\n",
    "5. Note que durante a simulação o `Episode Length` se manteve constante ao longo dos episódios. O que isso significa do ponto de vista da tarefa de RL?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
