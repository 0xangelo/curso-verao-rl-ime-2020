{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 4 - Parte prática - Actor-Critic (A2C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "Nesse quarto notebook ...\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(\\mathbf{a}_t|\\mathbf{s}_t) \\left ( \\left( \\sum_{k=t}^{T-1} r_{k} \\right)  - b(\\mathbf{s}_{t}) \\right ) \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "### Objetivos:\n",
    "\n",
    "- ambientes vetorizados (gym.vector.make)\n",
    "- retornos descontados\n",
    "- 2-head net / joint\n",
    "- entropy bonus\n",
    "- n-step return (bootstrap)\n",
    "- clip gradient by norm\n",
    "- learning_rate linear scheduler (decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from utils.agent import RLAgent\n",
    "from utils.memory import OnPolicyReplay\n",
    "from utils.networks import build_actor_critic_network\n",
    "import utils.runner\n",
    "from utils.viz import *\n",
    "\n",
    "\n",
    "tf.get_logger().setLevel(\"ERROR\")     # ignore TensorFlow warnings\n",
    "gym.logger.set_level(logging.ERROR)   # ignore OpenAI Gym warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C(RLAgent):\n",
    "    \n",
    "    def __init__(self, obs_space, action_space, config):\n",
    "        super().__init__(obs_space, action_space, config)\n",
    "        \n",
    "        self.memory = OnPolicyReplay()\n",
    "        self.actor_critic = build_actor_critic_network(obs_space, action_space, config[\"actor_critic_net\"])\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.RMSprop(\n",
    "            learning_rate=tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "                0.00083,\n",
    "                decay_steps=config[\"total_timesteps\"] / config[\"train_batch_size\"],\n",
    "                end_learning_rate=1e-4,\n",
    "                power=1.0\n",
    "            ))\n",
    "\n",
    "    def act(self, obs):\n",
    "        \"\"\"\n",
    "        Escolhe uma ação para ser tomada dada uma observação do ambiente.\n",
    "        \n",
    "        Args: \n",
    "            obs: observação do ambiente.\n",
    "        \n",
    "        Return:\n",
    "            action: ação válida dentro do espaço de ações.\n",
    "        \"\"\"\n",
    "        return self._act(obs).numpy()\n",
    "        \n",
    "    @tf.function\n",
    "    def _act(self, obs):\n",
    "        action_dist, _ = self.actor_critic(obs)\n",
    "        return action_dist.sample()\n",
    "    \n",
    "    def observe(self, obs, action, reward, next_obs, done):\n",
    "        \"\"\"\n",
    "        Registra na memória do agente uma transição do ambiente.\n",
    "\n",
    "        Args:\n",
    "            obs:            observação do ambiente antes da execução da ação.\n",
    "            action:         ação escolhida pelo agente.\n",
    "            reward (float): escalar indicando a recompensa obtida após a execução da ação.\n",
    "            next_obs:       nova observação recebida do ambiente após a execução da ação.\n",
    "            done (bool):    True se a nova observação corresponde a um estado terminal, False caso contrário.\n",
    "\n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.memory.update(obs, action, reward, next_obs, done)\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Método de treinamento do agente. A partir das experiências de sua memória,\n",
    "        o agente aprende um novo comportamento.\n",
    "\n",
    "        Args: \n",
    "            None\n",
    "\n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if self.memory.batch_size < self.config[\"train_batch_size\"]:\n",
    "            return\n",
    "        \n",
    "        batch = self.memory.sample()\n",
    "        weights = self.actor_critic.trainable_weights\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            policy_loss, vf_loss, entropy_loss = self._joint_loss_fn(batch)\n",
    "            loss = policy_loss + self.config[\"vf_loss_coeff\"] * tf.cast(vf_loss, tf.float32) - self.config[\"entropy_coeff\"] * entropy_loss\n",
    "            gradients = tape.gradient(loss, weights)\n",
    "    \n",
    "        gradients = tuple(tf.clip_by_norm(grad, clip_norm=0.5) for grad in gradients)\n",
    "        self.optimizer.apply_gradients(zip(gradients, weights))\n",
    "      \n",
    "        return {\n",
    "            \"policy_loss\": policy_loss.numpy(),\n",
    "            \"vf_loss\": vf_loss.numpy(),\n",
    "            \"entropy_loss\": entropy_loss.numpy()\n",
    "        }\n",
    "\n",
    "    def _joint_loss_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Calcula a função loss do policy gradients para um `batch` de transições.\n",
    "        \n",
    "        Um `batch` agrega arrays n-dimensionais. Cada array (e.g., batch[\"states\"],\n",
    "        batch[\"actions\"], batch[\"rewards\"]) tem como primeiras duas dimensões o número\n",
    "        de passos dados no ambiente vetorizado e o número de ambientes em paralelo. \n",
    "        Por exemplo, batch[\"states\"][t][k] devolve um array correspondendo ao estado \n",
    "        no passo t devolvido pelo k-ésimo ambiente.\n",
    "\n",
    "        Args:\n",
    "            batch (Dict[str, np.ndarray]): dicionário para acesso às matrizes de \n",
    "                estados, ações, recompensas, próximos estados e flags de terminação. \n",
    "        \n",
    "        Return:\n",
    "            loss (tf.Tensor): surrogate loss conjunta da política, função valor e\n",
    "                bônus de entropia.\n",
    "        \"\"\"\n",
    "        vf_criterion = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "        states = batch[\"states\"]\n",
    "        actions = batch[\"actions\"]\n",
    "        rewards = batch[\"rewards\"]\n",
    "        next_states = batch[\"next_states\"]\n",
    "        dones = batch[\"dones\"]\n",
    "\n",
    "        n_steps = len(states)\n",
    "        gamma = self.config[\"gamma\"]\n",
    "        lam = self.config[\"lambda\"]\n",
    "        \n",
    "\n",
    "        action_dists, values = self.actor_critic(states)\n",
    "        _, last_value = self.actor_critic(next_states[-1:])\n",
    "        \n",
    "        values = tf.squeeze(tf.concat([values, last_value], axis=0))\n",
    "        values, next_values = values[:-1], values[1:]\n",
    "\n",
    "        deltas = rewards + gamma * (1 - dones) * next_values - values\n",
    "\n",
    "        returns = np.empty_like(rewards)\n",
    "        advantages = np.empty_like(rewards)\n",
    "\n",
    "        returns[-1] = rewards[-1] + gamma * (1 - dones[-1]) * next_values[-1]\n",
    "        advantages[-1] = deltas[-1]\n",
    "\n",
    "        for t in reversed(range(n_steps - 1)):\n",
    "            returns[t] = rewards[t] + gamma * (1 - dones[t]) * returns[t+1]\n",
    "            advantages[t] = deltas[t] + (gamma * lam) * (1 - dones[t]) * advantages[t+1]\n",
    "\n",
    "        log_probs = action_dists.log_prob(actions)\n",
    "\n",
    "        policy_loss = - tf.reduce_sum(log_probs * tf.stop_gradient(advantages.astype(\"f\")))\n",
    "        vf_loss = vf_criterion(values, tf.stop_gradient(returns.astype(\"f\")))\n",
    "        entropy_loss = tf.reduce_mean(action_dists.entropy())\n",
    "\n",
    "        return policy_loss, vf_loss, entropy_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = 500_000\n",
    "\n",
    "num_envs = 8\n",
    "env = gym.vector.make(\"LunarLander-v2\", num_envs=num_envs, asynchronous=True)\n",
    "\n",
    "config = {\n",
    "    \"actor_critic_net\": {\n",
    "        \"hidden_layers\": [64, 64],\n",
    "        \"activation\": \"tanh\"\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"class_name\": \"RMSprop\",\n",
    "        \"config\": {\n",
    "            \"learning_rate\": 8e-4,\n",
    "            \"rho\": 0.99\n",
    "        }\n",
    "    },\n",
    "    \"total_timesteps\": total_timesteps,\n",
    "    \"train_batch_size\": 40,\n",
    "    \"gamma\": 0.995,\n",
    "    \"lambda\": 1.0,\n",
    "    \"vf_loss_coeff\": 0.25,\n",
    "    \"entropy_coeff\": 1e-5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = A2C(env.single_observation_space, env.single_action_space, config)\n",
    "\n",
    "tf.keras.utils.plot_model(agent.actor_critic, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps, total_rewards, avg_total_rewards = utils.runner.train(agent, env, total_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "for episode in range(20):\n",
    "    obs = env.reset()    \n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    while not done:\n",
    "        action = agent.act(obs[None,:])[0]\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        env.render()\n",
    "        obs = next_obs\n",
    "    env.close()\n",
    "    \n",
    "    print(f\"episode = {episode}, total_reward={total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
