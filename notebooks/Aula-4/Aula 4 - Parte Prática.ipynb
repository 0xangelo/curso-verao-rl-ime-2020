{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 4 - Parte prática - Actor-Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução \n",
    "\n",
    "Bla bla bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from utils.agent import RLAgent\n",
    "from utils.memory import MyReplayClass\n",
    "from utils.networks import build_discrete_policy, build_value_network\n",
    "import utils.runner\n",
    "from utils.viz import *\n",
    "\n",
    "\n",
    "tf.get_logger().setLevel(\"ERROR\")     # ignore TensorFlow warnings\n",
    "gym.logger.set_level(logging.ERROR)   # ignore OpenAI Gym warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C(RLAgent):\n",
    "    \n",
    "    def __init__(self, obs_space, action_space, config):\n",
    "        super().__init__(obs_space, action_space, config)\n",
    "        \n",
    "        self.memory = MyReplayClass()\n",
    "        self.policy = build_discrete_policy(self.obs_space, self.action_space, config[\"hidden_layers\"], config[\"activation\"])\n",
    "        vf_config = config[\"value_fn\"]\n",
    "        self.value_fn = build_value_network(obs_space, vf_config[\"hidden_layers\"], vf_config[\"activation\"])\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.get(config[\"optimizer\"])\n",
    "    \n",
    "    def act(self, obs):\n",
    "        \"\"\"\n",
    "        Escolhe uma ação para ser tomada dada uma observação do ambiente.\n",
    "        \n",
    "        Args: \n",
    "            obs: observação do ambiente.\n",
    "        \n",
    "        Return:\n",
    "            action: ação válida dentro do espaço de ações.\n",
    "        \"\"\"\n",
    "        return self._act(obs).numpy()\n",
    "    \n",
    "    @tf.function\n",
    "    def _act(self, obs):\n",
    "        action_dist = self.policy(obs)\n",
    "        return action_dist.sample()\n",
    "\n",
    "    def observe(self, obs, action, reward, next_obs, done):\n",
    "        \"\"\"\n",
    "        Registra na memória do agente uma transição do ambiente.\n",
    "\n",
    "        Args:\n",
    "            obs:            observação do ambiente antes da execução da ação.\n",
    "            action:         ação escolhida pelo agente.\n",
    "            reward (float): escalar indicando a recompensa obtida após a execução da ação.\n",
    "            next_obs:       nova observação recebida do ambiente após a execução da ação.\n",
    "            done (bool):    True se a nova observação corresponde a um estado terminal, False caso contrário.\n",
    "\n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.memory.update(obs, action, reward, next_obs, done)\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Método de treinamento do agente. A partir das experiências de sua memória,\n",
    "        o agente aprende um novo comportamento.\n",
    "\n",
    "        Args: \n",
    "            None\n",
    "\n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if self.memory.batch_size < self.config[\"train_batch_size\"]:\n",
    "            return\n",
    "        \n",
    "        batch = self.memory.sample()\n",
    "        weights = self.policy.trainable_weights + self.value_fn.trainable_weights\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._joint_loss_fn(batch)\n",
    "            gradients = tape.gradient(loss, weights)\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(gradients, weights))\n",
    "      \n",
    "        return loss\n",
    "\n",
    "    def _joint_loss_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Calcula a função loss do policy gradients para um `batch` de transições.\n",
    "        \n",
    "        Um `batch` agrega arrays n-dimensionais. Cada array (e.g., batch[\"states\"],\n",
    "        batch[\"actions\"], batch[\"rewards\"]) tem como primeiras duas dimensões o número\n",
    "        de passos dados no ambiente vetorizado e o número de ambientes em paralelo. \n",
    "        Por exemplo, batch[\"states\"][t][k] devolve um array correspondendo ao estado \n",
    "        no passo t devolvido pelo k-ésimo ambiente.\n",
    "\n",
    "        Args:\n",
    "            batch (Dict[str, np.ndarray]): dicionário para acesso às matrizes de \n",
    "                estados, ações, recompensas, próximos estados e flags de terminação. \n",
    "        \n",
    "        Return:\n",
    "            loss (tf.Tensor): surrogate loss conjunta da política, função valor e\n",
    "                bônus de entropia.\n",
    "        \"\"\"\n",
    "        vf_criterion = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "        states = batch[\"states\"]\n",
    "        actions = batch[\"actions\"]\n",
    "        rewards = batch[\"rewards\"]\n",
    "        next_states = batch[\"next_states\"]\n",
    "        dones = batch[\"dones\"]\n",
    "\n",
    "        n_steps = len(states)\n",
    "        gamma = self.config[\"gamma\"]\n",
    "        lam = self.config[\"lambda\"]\n",
    "        \n",
    "        values = self.value_fn(states)\n",
    "        next_values = self.value_fn(next_values)\n",
    "        deltas = rewards + gamma * (1 - dones) * next_values - values\n",
    "\n",
    "        returns = np.empty_like(rewards)\n",
    "        advantages = np.empty_like(rewards)\n",
    "        returns[-1] = rewards[-1] + gamma * (1 - dones) * next_values\n",
    "        advantages[-1] = deltas[-1]\n",
    "        for t in reversed(range(n_steps - 1)):\n",
    "            returns[t] = rewards[t] + gamma * (1 - dones[t]) * returns[t+1]\n",
    "            advantages[t] = deltas[t] + (gamma * lam) * (1 - dones[t]) * advantages[t+1]\n",
    "            \n",
    "        action_dists = self.policy(states)\n",
    "        log_probs = action_dists.log_prob(actions)\n",
    "        policy_loss = - tf.reduce_sum(log_probs * tf.stop_gradient(advantages))\n",
    "        vf_loss = vf_criterion(values, tf.stop_gradient(returns))\n",
    "        entropy_loss = tf.reduce_mean(action_dists.entropy())\n",
    "\n",
    "        return policy_loss + self.config[\"vf_loss_coeff\"] * vf_loss - self.config[\"entropy_coeff\"] * entropy_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
